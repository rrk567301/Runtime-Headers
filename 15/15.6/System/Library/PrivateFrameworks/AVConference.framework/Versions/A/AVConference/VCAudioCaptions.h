@class SFSpeechAnalyzerTranscriberOptions, NSObject, SFSpeechAnalyzer, VCCaptionTaskInfo, SFSpeechAudioBufferRecognitionRequest, NSLocale, SFSpeechRecognitionTask, SFSpeechAnalyzerOptions, NSMutableArray, NSString, NSSet, SFSpeechRecognizer, NSOperationQueue, AVAudioFormat;
@protocol OS_dispatch_queue, VCAudioCaptionsDelegate, OS_dispatch_semaphore;

@interface VCAudioCaptions : NSObject <SFSpeechRecognizerDelegate, SFSpeechRecognitionTaskDelegate, VCAudioIOSink, SFSpeechAnalyzerTranscriberResultDelegate, SFSpeechAnalyzerEndpointingResultDelegate> {
    char _isEnabled;
    char _isPrewarmed;
    char _isStarted;
    char _isLocal;
    char _useAnalyzerSpeechAPI;
    struct AudioStreamBasicDescription { double mSampleRate; unsigned int mFormatID; unsigned int mFormatFlags; unsigned int mBytesPerPacket; unsigned int mFramesPerPacket; unsigned int mBytesPerFrame; unsigned int mChannelsPerFrame; unsigned int mBitsPerChannel; unsigned int mReserved; } _inputFormat;
    unsigned int _lastVoiceAcitivty;
    int _recognizerState;
    SFSpeechRecognizer *_recognizer;
    SFSpeechAudioBufferRecognitionRequest *_recognizerRequest;
    SFSpeechRecognitionTask *_recognizerTask;
    VCCaptionTaskInfo *_currentTaskInfo;
    SFSpeechAnalyzer *_analyzer;
    SFSpeechAnalyzerTranscriberOptions *_transcriberOptions;
    SFSpeechAnalyzerOptions *_analyzerOptions;
    NSOperationQueue *_operationQueue;
    AVAudioFormat *_analyzerAudioFormat;
    NSObject<OS_dispatch_semaphore> *_analyzerAllResultsSemaphore;
    NSObject<OS_dispatch_queue> *_captionsQueue;
    NSObject<OS_dispatch_queue> *_delegateQueue;
    struct __CFAllocator { } *_copyBufferAllocator;
    struct __CFAllocator { } *_audioBufferAllocator;
    struct __CFAllocator { } *_pcmCopyBufferAllocator;
    struct __CFAllocator { } *_pcmAudioBufferAllocator;
    long long _currentTime;
    int _timescale;
    long long _epoch;
    NSLocale *_locale;
    id<VCAudioCaptionsDelegate> _delegate;
    unsigned long long _captioningRequestCount;
    struct opaqueRTCReporting { } *_reportingAgent;
    double _captionsEnabledDuration;
    double _lastCaptionsEnabledTime;
    struct { long long value; int timescale; unsigned int flags; long long epoch; } _lastAudioProcessedTime;
    double _captionedAudioDuration;
    unsigned int _captionTaskCount;
    double _captionsLastUtteranceStart;
    double _captionsUtteranceDuration;
    void *_previousConverterSamples;
    struct OpaqueAudioConverter { } *_audioConverter;
    char _isAudioConverterActive;
    struct AudioStreamBasicDescription { double mSampleRate; unsigned int mFormatID; unsigned int mFormatFlags; unsigned int mBytesPerPacket; unsigned int mFramesPerPacket; unsigned int mBytesPerFrame; unsigned int mChannelsPerFrame; unsigned int mBitsPerChannel; unsigned int mReserved; } _captionsFormat;
    unsigned int _currentUtteranceNumber;
    NSObject<OS_dispatch_semaphore> *_teardownSemaphore;
    char _inputFormatDidChange;
    NSMutableArray *_captionTasks;
    long long _currentActiveToken;
    void *_logCaptionsDump;
    char _isCaptionsDebugDumpEnabled;
    char _isSpeechModelLoaded;
    unsigned int _logMessageCounter;
}

@property (nonatomic) id<VCAudioCaptionsDelegate> delegate;
@property (copy, nonatomic) NSLocale *locale;
@property (readonly, nonatomic) char enabled;
@property (retain, nonatomic) NSSet *localLanguages;
@property (retain, nonatomic) NSSet *remoteLanguages;
@property (nonatomic) char remoteCanDisplay;
@property (retain, nonatomic) NSString *taskIdentifier;
@property (readonly) unsigned long long hash;
@property (readonly) Class superclass;
@property (readonly, copy) NSString *description;
@property (readonly, copy) NSString *debugDescription;

+ (char)captionsSupportedWithErrorCode:(long long *)a0;
+ (char)captionsSupported;
+ (char)shouldAllocateNewAllocator:(void *)a0 streamDesc:(const struct AudioStreamBasicDescription { double x0; unsigned int x1; unsigned int x2; unsigned int x3; unsigned int x4; unsigned int x5; unsigned int x6; unsigned int x7; unsigned int x8; } *)a1 referenceStreamDesc:(const struct AudioStreamBasicDescription { double x0; unsigned int x1; unsigned int x2; unsigned int x3; unsigned int x4; unsigned int x5; unsigned int x6; unsigned int x7; unsigned int x8; } *)a2;

- (void)dealloc;
- (void)stop;
- (void)stopWithCompletionHandler:(id /* block */)a0;
- (id)taskInfoForTask:(id)a0;
- (void)speechRecognitionDidDetectSpeech:(id)a0;
- (void)speechRecognitionTask:(id)a0 didFinishRecognition:(id)a1;
- (void)speechRecognitionTask:(id)a0 didFinishSuccessfully:(char)a1;
- (void)speechRecognitionTask:(id)a0 didHypothesizeTranscription:(id)a1;
- (void)speechRecognitionTaskWasCancelled:(id)a0;
- (void)speechRecognizer:(id)a0 availabilityDidChange:(char)a1;
- (void)destroyAnalyzer;
- (void)enableCaptions:(char)a0;
- (void)prewarmCaptions;
- (char)recognizerBufferSetupWithError:(id *)a0;
- (char)analyzerSetupWithError:(id *)a0;
- (void)analyzerTeardown;
- (void)callCompletionHandler:(id /* block */)a0 withResult:(char)a1;
- (char)captionsDebugDumpEnabled;
- (char)configureAnalyzerOptions;
- (struct opaqueCMSampleBuffer { } *)convertSamples:(char *)a0 numSamples:(int)a1;
- (char)createAudioConverterWithInputFormat:(const struct AudioStreamBasicDescription { double x0; unsigned int x1; unsigned int x2; unsigned int x3; unsigned int x4; unsigned int x5; unsigned int x6; unsigned int x7; unsigned int x8; } *)a0 outputFormat:(const struct AudioStreamBasicDescription { double x0; unsigned int x1; unsigned int x2; unsigned int x3; unsigned int x4; unsigned int x5; unsigned int x6; unsigned int x7; unsigned int x8; } *)a1 converter:(struct OpaqueAudioConverter **)a2;
- (char)createRecognizer:(id *)a0;
- (struct opaqueCMSampleBuffer { } *)createSampleBufferWithFormat:(const struct AudioStreamBasicDescription { double x0; unsigned int x1; unsigned int x2; unsigned int x3; unsigned int x4; unsigned int x5; unsigned int x6; unsigned int x7; unsigned int x8; } *)a0 samples:(char *)a1 numSamples:(int)a2;
- (void)destroyRecognizer;
- (void)dumpCaptionsIfNeededForCaptionsTranscription:(id)a0 final:(char)a1;
- (void)dumpCaptionsIfNeededForTranscription:(id)a0 final:(char)a1;
- (void)gatherRealtimeStats:(struct __CFDictionary { } *)a0;
- (char)handleStateLoadedError:(id *)a0;
- (void)handleStateStopping;
- (char)idleStateToState:(int)a0 withReason:(int)a1 error:(id *)a2;
- (id)initWithDelegate:(id)a0 isLocal:(char)a1 taskIdentifier:(id)a2 reportingAgent:(struct opaqueRTCReporting { } *)a3;
- (char)loadedStateToState:(int)a0 withReason:(int)a1 error:(id *)a2;
- (id)newPCMSampleBufferWithSamples:(char *)a0 numSamples:(int)a1;
- (void)packageAndSendTranscribedString:(id)a0 withTask:(id)a1 final:(char)a2;
- (void)packageAndSendTranscriberResult:(id)a0 withTask:(id)a1 final:(char)a2;
- (void)pushAudioSamples:(struct opaqueVCAudioBufferList { } *)a0;
- (void)pushSamplesToAnalyzer:(char *)a0 numSamples:(int)a1 hostTime:(double)a2;
- (void)pushSamplesToRecognizer:(char *)a0 numSamples:(int)a1 hostTime:(double)a2;
- (char)reallocCopyBufferAllocatorWithFormat:(const struct AudioStreamBasicDescription { double x0; unsigned int x1; unsigned int x2; unsigned int x3; unsigned int x4; unsigned int x5; unsigned int x6; unsigned int x7; unsigned int x8; } *)a0;
- (void)recognizerBufferTeardown;
- (void)recordAudioSampleMetrics;
- (char)runningStateToState:(int)a0 withReason:(int)a1 error:(id *)a2;
- (char)shouldPushSamplesToAnalyzer;
- (char)shouldPushSamplesToRecognizer;
- (void)speechAnalyzer:(id)a0 didProduceEndpointingResult:(id)a1;
- (void)speechAnalyzer:(id)a0 didProduceTranscriberResult:(id)a1;
- (void)speechAnalyzer:(id)a0 didStopEndpointingWithError:(id)a1;
- (void)speechAnalyzer:(id)a0 didStopTranscriptionWithError:(id)a1;
- (void)speechAnalyzerDidProduceAllTranscriberResults:(id)a0;
- (void)start:(const struct AudioStreamBasicDescription { double x0; unsigned int x1; unsigned int x2; unsigned int x3; unsigned int x4; unsigned int x5; unsigned int x6; unsigned int x7; unsigned int x8; } *)a0 forToken:(long long)a1 withCompletionHandler:(id /* block */)a2;
- (char)stoppingStateToState:(int)a0 withReason:(int)a1 error:(id *)a2;
- (char)transitionToState:(int)a0 withReason:(int)a1 error:(id *)a2;

@end
